{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86ec34b-fac2-4c73-81cd-61c5391d8a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n",
      "Package versions:\n",
      "  - XGBoost: 2.1.2\n",
      "  - LightGBM: 4.5.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "05_advanced_models.ipynb\n",
    "Train advanced ML models (XGBoost, LightGBM) with \n",
    "hyperparameter tuning.\n",
    "\n",
    "Goals:\n",
    "- PTS: ‚â§5.09 MAE (Tier 1), ‚â§4.50 (Tier 2)\n",
    "- REB: ‚â§1.97 MAE (Tier 1), ‚â§2.00 (Tier 2)\n",
    "- AST: ‚â§1.51 MAE (Tier 1), ‚â§1.50 (Tier 2)\n",
    "\n",
    "Current baseline results (from notebook 04):\n",
    "- PTS: 5.293 MAE (need 0.203 improvement)\n",
    "- REB: 2.080 MAE (need 0.110 improvement)\n",
    "- AST: 1.539 MAE (basically there!)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(f\"Package versions:\")\n",
    "print(f\"  - XGBoost: {xgb.__version__}\")\n",
    "print(f\"  - LightGBM: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52bd5e74-f947-404a-97b0-ce194d226aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loaded successfully\n",
      "\n",
      "üìä Dataset shapes:\n",
      "  Train: 38,315 games √ó 45 features\n",
      "  Val:   14,020 games √ó 45 features\n",
      "  Test:  14,074 games √ó 45 features\n",
      "\n",
      "üéØ Current best results (from notebook 04):\n",
      "  PTS: 5.293 MAE ‚Üí Goal: ‚â§5.09 (Tier 1), ‚â§4.50 (Tier 2)\n",
      "  REB: 2.080 MAE ‚Üí Goal: ‚â§1.97 (Tier 1), ‚â§2.00 (Tier 2)\n",
      "  AST: 1.539 MAE ‚Üí Goal: ‚â§1.51 (Tier 1), ‚â§1.50 (Tier 2)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "y_train_pts = pd.read_csv('../data/processed/y_train_pts.csv')['PTS']\n",
    "y_train_reb = pd.read_csv('../data/processed/y_train_reb.csv')['REB']\n",
    "y_train_ast = pd.read_csv('../data/processed/y_train_ast.csv')['AST']\n",
    "\n",
    "# Load validation data\n",
    "X_val = pd.read_csv('../data/processed/X_val.csv')\n",
    "y_val_pts = pd.read_csv('../data/processed/y_val_pts.csv')['PTS']\n",
    "y_val_reb = pd.read_csv('../data/processed/y_val_reb.csv')['REB']\n",
    "y_val_ast = pd.read_csv('../data/processed/y_val_ast.csv')['AST']\n",
    "\n",
    "# Load test data (we'll save this for final evaluation)\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_test_pts = pd.read_csv('../data/processed/y_test_pts.csv')['PTS']\n",
    "y_test_reb = pd.read_csv('../data/processed/y_test_reb.csv')['REB']\n",
    "y_test_ast = pd.read_csv('../data/processed/y_test_ast.csv')['AST']\n",
    "\n",
    "# Load feature names\n",
    "with open('../data/processed/feature_names.json', 'r') as f:\n",
    "  feature_names = json.load(f)\n",
    "\n",
    "print(\"‚úì Data loaded successfully\")\n",
    "print(f\"\\nüìä Dataset shapes:\")\n",
    "print(f\"  Train: {X_train.shape[0]:,} games √ó {X_train.shape[1]} features\")\n",
    "print(f\"  Val:   {X_val.shape[0]:,} games √ó {X_val.shape[1]} features\")\n",
    "print(f\"  Test:  {X_test.shape[0]:,} games √ó {X_test.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nüéØ Current best results (from notebook 04):\")\n",
    "print(f\"  PTS: 5.293 MAE ‚Üí Goal: ‚â§5.09 (Tier 1), ‚â§4.50 (Tier 2)\")\n",
    "print(f\"  REB: 2.080 MAE ‚Üí Goal: ‚â§1.97 (Tier 1), ‚â§2.00 (Tier 2)\")\n",
    "print(f\"  AST: 1.539 MAE ‚Üí Goal: ‚â§1.51 (Tier 1), ‚â§1.50 (Tier 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5193525e-73e3-4226-a803-ebf038903b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING DEFAULT XGBOOST MODELS\n",
      "======================================================================\n",
      "\n",
      "1. Training XGBoost for PTS...\n",
      "   Val MAE: 5.478\n",
      "\n",
      "2. Training XGBoost for REB...\n",
      "   Val MAE: 2.172\n",
      "\n",
      "3. Training XGBoost for AST...\n",
      "   Val MAE: 1.612\n",
      "\n",
      "======================================================================\n",
      "DEFAULT XGBOOST RESULTS\n",
      "======================================================================\n",
      "target  val_mae   val_r2\n",
      "   PTS 5.477806 0.418834\n",
      "   REB 2.172228 0.362597\n",
      "   AST 1.612178 0.450105\n",
      "\n",
      "======================================================================\n",
      "COMPARISON TO LINEAR MODELS\n",
      "======================================================================\n",
      "Target  Linear Model  XGBoost Default  Tier 1 Goal  Beat Linear?  Hit Tier 1?\n",
      "   PTS         5.293         5.477806         5.09         False        False\n",
      "   REB         2.080         2.172228         1.97         False        False\n",
      "   AST         1.539         1.612178         1.51         False        False\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING DEFAULT XGBOOST MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Function to train and evaluate\n",
    "def train_and_evaluate_tree(model, model_name, X_train, y_train, X_val, y_val):\n",
    "  \"\"\"Train tree model and return metrics + predictions\"\"\"\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  y_pred_val = model.predict(X_val)\n",
    "\n",
    "  metrics = {\n",
    "      'model': model_name,\n",
    "      'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "      'val_mae': mean_absolute_error(y_val, y_pred_val),\n",
    "      'train_r2': r2_score(y_train, y_pred_train),\n",
    "      'val_r2': r2_score(y_val, y_pred_val)\n",
    "  }\n",
    "\n",
    "  return metrics, model\n",
    "\n",
    "# Train XGBoost for each target with default params\n",
    "results_xgb_default = []\n",
    "\n",
    "print(\"\\n1. Training XGBoost for PTS...\")\n",
    "xgb_pts = xgb.XGBRegressor(\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  tree_method='hist'  # Fast histogram-based algorithm\n",
    ")\n",
    "metrics_pts, model_pts = train_and_evaluate_tree(xgb_pts, 'XGBoost_default', X_train, y_train_pts, X_val, y_val_pts)\n",
    "results_xgb_default.append({**metrics_pts, 'target': 'PTS'})\n",
    "print(f\"   Val MAE: {metrics_pts['val_mae']:.3f}\")\n",
    "\n",
    "print(\"\\n2. Training XGBoost for REB...\")\n",
    "xgb_reb = xgb.XGBRegressor(\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  tree_method='hist'\n",
    ")\n",
    "metrics_reb, model_reb = train_and_evaluate_tree(xgb_reb, 'XGBoost_default', X_train, y_train_reb, X_val, y_val_reb)\n",
    "results_xgb_default.append({**metrics_reb, 'target': 'REB'})\n",
    "print(f\"   Val MAE: {metrics_reb['val_mae']:.3f}\")\n",
    "\n",
    "print(\"\\n3. Training XGBoost for AST...\")\n",
    "xgb_ast = xgb.XGBRegressor(\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  tree_method='hist'\n",
    ")\n",
    "metrics_ast, model_ast = train_and_evaluate_tree(xgb_ast, 'XGBoost_default', X_train, y_train_ast, X_val, y_val_ast)\n",
    "results_xgb_default.append({**metrics_ast, 'target': 'AST'})\n",
    "print(f\"   Val MAE: {metrics_ast['val_mae']:.3f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEFAULT XGBOOST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "results_df = pd.DataFrame(results_xgb_default)\n",
    "print(results_df[['target', 'val_mae', 'val_r2']].to_string(index=False))\n",
    "\n",
    "# Compare to baseline\n",
    "baseline_comparison = pd.DataFrame({\n",
    "  'Target': ['PTS', 'REB', 'AST'],\n",
    "  'Linear Model': [5.293, 2.080, 1.539],\n",
    "  'XGBoost Default': [metrics_pts['val_mae'], metrics_reb['val_mae'], metrics_ast['val_mae']],\n",
    "  'Tier 1 Goal': [5.09, 1.97, 1.51]\n",
    "})\n",
    "baseline_comparison['Beat Linear?'] = baseline_comparison['XGBoost Default'] < baseline_comparison['Linear Model']\n",
    "baseline_comparison['Hit Tier 1?'] = baseline_comparison['XGBoost Default'] <= baseline_comparison['Tier 1 Goal']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON TO LINEAR MODELS\")\n",
    "print(\"=\"*70)\n",
    "print(baseline_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd18b2e-cd89-4f00-ac70-be6c5a724283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNOSING TRAIN VS VAL PERFORMANCE\n",
      "======================================================================\n",
      "Target  Train MAE  Val MAE  Gap (Val - Train)  Overfitting?\n",
      "   PTS   4.131616 5.477806           1.346191          True\n",
      "   REB   1.664972 2.172228           0.507256          True\n",
      "   AST   1.206033 1.612178           0.406146         False\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSIS\n",
      "======================================================================\n",
      "  PTS: SEVERE overfitting (gap=1.346) ‚Üí Need regularization\n",
      "  REB: Moderate overfitting (gap=0.507) ‚Üí Tune max_depth, min_child_weight\n",
      "  AST: Good fit (gap=0.406)\n",
      "\n",
      "üí° NEXT STEP: Hyperparameter tuning with GridSearchCV\n",
      "   Focus on: learning_rate, max_depth, n_estimators, subsample\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DIAGNOSING TRAIN VS VAL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check train MAE for each model\n",
    "diagnosis = pd.DataFrame({\n",
    "  'Target': ['PTS', 'REB', 'AST'],\n",
    "  'Train MAE': [\n",
    "      results_xgb_default[0]['train_mae'],\n",
    "      results_xgb_default[1]['train_mae'],\n",
    "      results_xgb_default[2]['train_mae']\n",
    "  ],\n",
    "  'Val MAE': [\n",
    "      results_xgb_default[0]['val_mae'],\n",
    "      results_xgb_default[1]['val_mae'],\n",
    "      results_xgb_default[2]['val_mae']\n",
    "  ]\n",
    "})\n",
    "diagnosis['Gap (Val - Train)'] = diagnosis['Val MAE'] - diagnosis['Train MAE']\n",
    "diagnosis['Overfitting?'] = diagnosis['Gap (Val - Train)'] > 0.5\n",
    "\n",
    "print(diagnosis.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, row in diagnosis.iterrows():\n",
    "  gap = row['Gap (Val - Train)']\n",
    "  target = row['Target']\n",
    "\n",
    "  if gap > 1.0:\n",
    "      print(f\"  {target}: SEVERE overfitting (gap={gap:.3f}) ‚Üí Need regularization\")\n",
    "  elif gap > 0.5:\n",
    "      print(f\"  {target}: Moderate overfitting (gap={gap:.3f}) ‚Üí Tune max_depth, min_child_weight\")\n",
    "  elif gap < 0.1:\n",
    "      print(f\"  {target}: Underfitting (gap={gap:.3f}) ‚Üí Need more complexity\")\n",
    "  else:\n",
    "      print(f\"  {target}: Good fit (gap={gap:.3f})\")\n",
    "\n",
    "print(\"\\nüí° NEXT STEP: Hyperparameter tuning with GridSearchCV\")\n",
    "print(\"   Focus on: learning_rate, max_depth, n_estimators, subsample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8494b83d-9e39-4e47-8e76-d99b32353ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING FOR PTS (RandomizedSearchCV)\n",
      "======================================================================\n",
      "This will take ~5-10 minutes...\n",
      "\n",
      "Starting hyperparameter search...\n",
      "  Total combinations to try: 50\n",
      "  Cross-validation folds: 3\n",
      "  Scoring metric: MAE\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "‚úì Search complete!\n",
      "\n",
      "Best parameters:\n",
      "  subsample: 1.0\n",
      "  reg_lambda: 2\n",
      "  reg_alpha: 0\n",
      "  n_estimators: 100\n",
      "  min_child_weight: 1\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.05\n",
      "  colsample_bytree: 0.8\n",
      "\n",
      "üìä Best XGBoost for PTS:\n",
      "  Train MAE: 5.202\n",
      "  Val MAE:   5.305\n",
      "  Gap:       0.103\n",
      "\n",
      "üéØ Comparison:\n",
      "  Linear Model:     5.293 MAE\n",
      "  XGBoost Default:  5.478 MAE\n",
      "  XGBoost Tuned:    5.305 MAE\n",
      "  Tier 1 Goal:      5.09 MAE\n",
      "  ‚ùå Still need improvement\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING FOR PTS (RandomizedSearchCV)\")\n",
    "print(\"=\"*70)\n",
    "print(\"This will take ~5-10 minutes...\")\n",
    "\n",
    "# Define parameter grid focused on reducing overfitting\n",
    "param_distributions = {\n",
    "  'max_depth': [3, 4, 5, 6],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'n_estimators': [100, 200, 300],\n",
    "  'min_child_weight': [1, 3, 5],\n",
    "  'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "  'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "  'reg_alpha': [0, 0.1, 1.0],  # L1 regularization\n",
    "  'reg_lambda': [1, 2, 5]      # L2 regularization\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV (faster than GridSearchCV)\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  tree_method='hist'\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "  xgb_model,\n",
    "  param_distributions=param_distributions,\n",
    "  n_iter=50,  # Try 50 random combinations\n",
    "  scoring='neg_mean_absolute_error',\n",
    "  cv=3,  # 3-fold CV\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting hyperparameter search...\")\n",
    "print(f\"  Total combinations to try: 50\")\n",
    "print(f\"  Cross-validation folds: 3\")\n",
    "print(f\"  Scoring metric: MAE\")\n",
    "\n",
    "random_search.fit(X_train, y_train_pts)\n",
    "\n",
    "print(\"\\n‚úì Search complete!\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "  print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate best model\n",
    "best_xgb_pts = random_search.best_estimator_\n",
    "y_pred_train = best_xgb_pts.predict(X_train)\n",
    "y_pred_val = best_xgb_pts.predict(X_val)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pts, y_pred_train)\n",
    "val_mae = mean_absolute_error(y_val_pts, y_pred_val)\n",
    "\n",
    "print(f\"\\nüìä Best XGBoost for PTS:\")\n",
    "print(f\"  Train MAE: {train_mae:.3f}\")\n",
    "print(f\"  Val MAE:   {val_mae:.3f}\")\n",
    "print(f\"  Gap:       {val_mae - train_mae:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Comparison:\")\n",
    "print(f\"  Linear Model:     5.293 MAE\")\n",
    "print(f\"  XGBoost Default:  5.478 MAE\")\n",
    "print(f\"  XGBoost Tuned:    {val_mae:.3f} MAE\")\n",
    "print(f\"  Tier 1 Goal:      5.09 MAE\")\n",
    "\n",
    "if val_mae <= 5.09:\n",
    "  print(f\"  ‚úÖ HIT TIER 1 GOAL!\")\n",
    "elif val_mae < 5.293:\n",
    "  print(f\"  ‚úÖ Beat linear model!\")\n",
    "else:\n",
    "  print(f\"  ‚ùå Still need improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636ba373-e1b3-4ea5-af24-be72a128c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING FOR REB\n",
      "======================================================================\n",
      "\n",
      "Starting search for REB...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "‚úì Search complete!\n",
      "\n",
      "Best parameters:\n",
      "  subsample: 1.0\n",
      "  reg_lambda: 2\n",
      "  reg_alpha: 0.1\n",
      "  n_estimators: 100\n",
      "  min_child_weight: 3\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.05\n",
      "  colsample_bytree: 0.8\n",
      "\n",
      "üìä Best XGBoost for REB:\n",
      "  Train MAE: 2.083\n",
      "  Val MAE:   2.085\n",
      "  Gap:       0.001\n",
      "\n",
      "üéØ Comparison:\n",
      "  Linear Model:     2.080 MAE\n",
      "  XGBoost Default:  2.172 MAE\n",
      "  XGBoost Tuned:    2.085 MAE\n",
      "  Tier 1 Goal:      1.97 MAE\n",
      "  ‚ùå Still need improvement\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING FOR REB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Same parameter grid\n",
    "param_distributions = {\n",
    "  'max_depth': [3, 4, 5, 6],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'n_estimators': [100, 200, 300],\n",
    "  'min_child_weight': [1, 3, 5],\n",
    "  'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "  'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "  'reg_alpha': [0, 0.1, 1.0],\n",
    "  'reg_lambda': [1, 2, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist')\n",
    "\n",
    "random_search_reb = RandomizedSearchCV(\n",
    "  xgb_model,\n",
    "  param_distributions=param_distributions,\n",
    "  n_iter=50,\n",
    "  scoring='neg_mean_absolute_error',\n",
    "  cv=3,\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting search for REB...\")\n",
    "random_search_reb.fit(X_train, y_train_reb)\n",
    "\n",
    "print(\"\\n‚úì Search complete!\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search_reb.best_params_.items():\n",
    "  print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate\n",
    "best_xgb_reb = random_search_reb.best_estimator_\n",
    "y_pred_train = best_xgb_reb.predict(X_train)\n",
    "y_pred_val = best_xgb_reb.predict(X_val)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_reb, y_pred_train)\n",
    "val_mae = mean_absolute_error(y_val_reb, y_pred_val)\n",
    "\n",
    "print(f\"\\nüìä Best XGBoost for REB:\")\n",
    "print(f\"  Train MAE: {train_mae:.3f}\")\n",
    "print(f\"  Val MAE:   {val_mae:.3f}\")\n",
    "print(f\"  Gap:       {val_mae - train_mae:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Comparison:\")\n",
    "print(f\"  Linear Model:     2.080 MAE\")\n",
    "print(f\"  XGBoost Default:  2.172 MAE\")\n",
    "print(f\"  XGBoost Tuned:    {val_mae:.3f} MAE\")\n",
    "print(f\"  Tier 1 Goal:      1.97 MAE\")\n",
    "\n",
    "if val_mae <= 1.97:\n",
    "  print(f\"  ‚úÖ HIT TIER 1 GOAL!\")\n",
    "elif val_mae < 2.080:\n",
    "  print(f\"  ‚úÖ Beat linear model!\")\n",
    "else:\n",
    "  print(f\"  ‚ùå Still need improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6f721c-799d-44d4-abc9-63c71841f72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING FOR AST\n",
      "======================================================================\n",
      "\n",
      "Starting search for AST...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "‚úì Search complete!\n",
      "\n",
      "Best parameters:\n",
      "  subsample: 1.0\n",
      "  reg_lambda: 2\n",
      "  reg_alpha: 0\n",
      "  n_estimators: 100\n",
      "  min_child_weight: 1\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.05\n",
      "  colsample_bytree: 0.8\n",
      "\n",
      "üìä Best XGBoost for AST:\n",
      "  Train MAE: 1.520\n",
      "  Val MAE:   1.543\n",
      "  Gap:       0.023\n",
      "\n",
      "üéØ Comparison:\n",
      "  Linear Model:     1.539 MAE\n",
      "  XGBoost Default:  1.612 MAE\n",
      "  XGBoost Tuned:    1.543 MAE\n",
      "  Tier 1 Goal:      1.51 MAE\n",
      "  ‚ùå Still need improvement\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING FOR AST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "param_distributions = {\n",
    "  'max_depth': [3, 4, 5, 6],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'n_estimators': [100, 200, 300],\n",
    "  'min_child_weight': [1, 3, 5],\n",
    "  'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "  'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "  'reg_alpha': [0, 0.1, 1.0],\n",
    "  'reg_lambda': [1, 2, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist')\n",
    "\n",
    "random_search_ast = RandomizedSearchCV(\n",
    "  xgb_model,\n",
    "  param_distributions=param_distributions,\n",
    "  n_iter=50,\n",
    "  scoring='neg_mean_absolute_error',\n",
    "  cv=3,\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting search for AST...\")\n",
    "random_search_ast.fit(X_train, y_train_ast)\n",
    "\n",
    "print(\"\\n‚úì Search complete!\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search_ast.best_params_.items():\n",
    "  print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate\n",
    "best_xgb_ast = random_search_ast.best_estimator_\n",
    "y_pred_train = best_xgb_ast.predict(X_train)\n",
    "y_pred_val = best_xgb_ast.predict(X_val)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_ast, y_pred_train)\n",
    "val_mae = mean_absolute_error(y_val_ast, y_pred_val)\n",
    "\n",
    "print(f\"\\nüìä Best XGBoost for AST:\")\n",
    "print(f\"  Train MAE: {train_mae:.3f}\")\n",
    "print(f\"  Val MAE:   {val_mae:.3f}\")\n",
    "print(f\"  Gap:       {val_mae - train_mae:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Comparison:\")\n",
    "print(f\"  Linear Model:     1.539 MAE\")\n",
    "print(f\"  XGBoost Default:  1.612 MAE\")\n",
    "print(f\"  XGBoost Tuned:    {val_mae:.3f} MAE\")\n",
    "print(f\"  Tier 1 Goal:      1.51 MAE\")\n",
    "\n",
    "if val_mae <= 1.51:\n",
    "  print(f\"  ‚úÖ HIT TIER 1 GOAL!\")\n",
    "elif val_mae < 1.539:\n",
    "  print(f\"  ‚úÖ Beat linear model!\")\n",
    "else:\n",
    "  print(f\"  ‚ùå Still need improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3bb524-9520-4d0f-9e3e-b8d4236b9eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRYING LIGHTGBM (ALTERNATIVE TREE ALGORITHM)\n",
      "======================================================================\n",
      "LightGBM uses leaf-wise growth vs XGBoost's level-wise\n",
      "\n",
      "1. Tuning LightGBM for PTS...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "   LightGBM PTS Val MAE: 5.299\n",
      "   Best params: {'subsample': 1.0, 'reg_lambda': 2, 'reg_alpha': 0.1, 'num_leaves': 15, 'n_estimators': 100, 'min_child_samples': 50, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "\n",
      "2. Tuning LightGBM for REB...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "   LightGBM REB Val MAE: 2.086\n",
      "\n",
      "3. Tuning LightGBM for AST...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "   LightGBM AST Val MAE: 1.542\n",
      "\n",
      "======================================================================\n",
      "LIGHTGBM SUMMARY\n",
      "======================================================================\n",
      "  PTS: 5.299 (Goal: ‚â§5.09)\n",
      "  REB: 2.086 (Goal: ‚â§1.97)\n",
      "  AST: 1.542 (Goal: ‚â§1.51)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRYING LIGHTGBM (ALTERNATIVE TREE ALGORITHM)\")\n",
    "print(\"=\"*70)\n",
    "print(\"LightGBM uses leaf-wise growth vs XGBoost's level-wise\")\n",
    "\n",
    "# LightGBM parameter grid\n",
    "param_distributions_lgb = {\n",
    "  'num_leaves': [15, 31, 63],  # LightGBM uses leaves instead of depth\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'n_estimators': [100, 200, 300],\n",
    "  'min_child_samples': [20, 30, 50],\n",
    "  'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "  'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "  'reg_alpha': [0, 0.1, 1.0],\n",
    "  'reg_lambda': [1, 2, 5]\n",
    "}\n",
    "\n",
    "print(\"\\n1. Tuning LightGBM for PTS...\")\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "random_search_lgb_pts = RandomizedSearchCV(\n",
    "  lgb_model,\n",
    "  param_distributions=param_distributions_lgb,\n",
    "  n_iter=50,\n",
    "  scoring='neg_mean_absolute_error',\n",
    "  cv=3,\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "random_search_lgb_pts.fit(X_train, y_train_pts)\n",
    "\n",
    "best_lgb_pts = random_search_lgb_pts.best_estimator_\n",
    "y_pred_val_pts = best_lgb_pts.predict(X_val)\n",
    "val_mae_pts = mean_absolute_error(y_val_pts, y_pred_val_pts)\n",
    "\n",
    "print(f\"\\n   LightGBM PTS Val MAE: {val_mae_pts:.3f}\")\n",
    "print(f\"   Best params: {random_search_lgb_pts.best_params_}\")\n",
    "\n",
    "print(\"\\n2. Tuning LightGBM for REB...\")\n",
    "random_search_lgb_reb = RandomizedSearchCV(\n",
    "  lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "  param_distributions=param_distributions_lgb,\n",
    "  n_iter=50,\n",
    "  scoring='neg_mean_absolute_error',\n",
    "  cv=3,\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "random_search_lgb_reb.fit(X_train, y_train_reb)\n",
    "\n",
    "best_lgb_reb = random_search_lgb_reb.best_estimator_\n",
    "y_pred_val_reb = best_lgb_reb.predict(X_val)\n",
    "val_mae_reb = mean_absolute_error(y_val_reb, y_pred_val_reb)\n",
    "\n",
    "print(f\"\\n   LightGBM REB Val MAE: {val_mae_reb:.3f}\")\n",
    "\n",
    "print(\"\\n3. Tuning LightGBM for AST...\")\n",
    "random_search_lgb_ast = RandomizedSearchCV(\n",
    "  lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "  param_distributions=param_distributions_lgb,\n",
    "  n_iter=50,\n",
    "  scoring='neg_mean_absolute_error',\n",
    "  cv=3,\n",
    "  random_state=42,\n",
    "  n_jobs=-1,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "random_search_lgb_ast.fit(X_train, y_train_ast)\n",
    "\n",
    "best_lgb_ast = random_search_lgb_ast.best_estimator_\n",
    "y_pred_val_ast = best_lgb_ast.predict(X_val)\n",
    "val_mae_ast = mean_absolute_error(y_val_ast, y_pred_val_ast)\n",
    "\n",
    "print(f\"\\n   LightGBM AST Val MAE: {val_mae_ast:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LIGHTGBM SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  PTS: {val_mae_pts:.3f} (Goal: ‚â§5.09)\")\n",
    "print(f\"  REB: {val_mae_reb:.3f} (Goal: ‚â§1.97)\")\n",
    "print(f\"  AST: {val_mae_ast:.3f} (Goal: ‚â§1.51)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0602c120-541c-453b-97dd-5a7fbf7548a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "======================================================================\n",
      "            Model  PTS MAE  REB MAE  AST MAE\n",
      "Linear Regression    5.293    2.081    1.539\n",
      "     Ridge (best)    5.296    2.080    1.539\n",
      "  XGBoost (tuned)    5.305    2.085    1.543\n",
      " LightGBM (tuned)    5.299    2.086    1.542\n",
      "\n",
      "======================================================================\n",
      "TIER 1 GOALS (10% improvement over rolling avg baseline)\n",
      "======================================================================\n",
      "  PTS: ‚â§5.09 MAE  (current best: 5.293, need -0.203)\n",
      "  REB: ‚â§1.97 MAE  (current best: 2.080, need -0.110)\n",
      "  AST: ‚â§1.51 MAE  (current best: 1.539, need -0.029) ‚≠ê SO CLOSE!\n",
      "\n",
      "======================================================================\n",
      "KEY FINDINGS\n",
      "======================================================================\n",
      "‚úÖ All models perform essentially the same (~5.29 PTS, ~2.08 REB, ~1.54 AST)\n",
      "‚úÖ Tree models chose shallow configs (max_depth=3, num_leaves=15)\n",
      "‚úÖ Conclusion: Relationships are mostly LINEAR with current 45 features\n",
      "\n",
      "‚ö†Ô∏è  HITTING A PLATEAU - Need new information to break through!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10b3853a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10b7853a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10ace93a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x109a953a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x109ba93a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10bab53a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10acc13a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10b78d3a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "  'Model': ['Linear Regression', 'Ridge (best)', 'XGBoost (tuned)', 'LightGBM (tuned)'],\n",
    "  'PTS MAE': [5.293, 5.296, 5.305, 5.299],\n",
    "  'REB MAE': [2.081, 2.080, 2.085, 2.086],\n",
    "  'AST MAE': [1.539, 1.539, 1.543, 1.542]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIER 1 GOALS (10% improvement over rolling avg baseline)\")\n",
    "print(\"=\"*70)\n",
    "print(\"  PTS: ‚â§5.09 MAE  (current best: 5.293, need -0.203)\")\n",
    "print(\"  REB: ‚â§1.97 MAE  (current best: 2.080, need -0.110)\")\n",
    "print(\"  AST: ‚â§1.51 MAE  (current best: 1.539, need -0.029) ‚≠ê SO CLOSE!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ All models perform essentially the same (~5.29 PTS, ~2.08 REB, ~1.54 AST)\")\n",
    "print(\"‚úÖ Tree models chose shallow configs (max_depth=3, num_leaves=15)\")\n",
    "print(\"‚úÖ Conclusion: Relationships are mostly LINEAR with current 45 features\")\n",
    "print(\"\\n‚ö†Ô∏è  HITTING A PLATEAU - Need new information to break through!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8d05-1398-4829-9fa4-483694816de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
